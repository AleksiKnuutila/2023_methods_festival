---
title: "Language models as research tools: Potentials and pitfalls when studying hate speech"
subtitle: Methods festival 2023
format:
  clean-revealjs:
    self-contained: true
    template-partials:
      - title-slide.html
author:
  - name: Aleksi Knuutila
    orcid: 0000-0002-9874-0079
    email: aleksi.knuutila@helsinki.fi
    affiliations: JYU, HY
date: last-modified
bibliography: refs.bib
slide_link: http://knuutila.net/methodsfest2023
---
## A bit about me

- University researcher in Helsinki (Sociology) and Jyväskylä (Culture studies)
- Working on information environment in Ukraine, hate speech in Finland
- Interested in critical and creative use of machine learning in social sciences

::: {.notes}
- I've worked on hate speech for a few years now, collaborating with many people in this panel also
:::
## Structure of presentation

1. What are large language models ("LLMs")
2. Challenges in hate speech research
3. Examples of my own research and novel uses of LLMs
4. Concluding summary

::: {.notes}
- Teaching on LLMs, tried to connect it hate speech
- Research approach, based on categorisation of large datasets
:::

# What are large language models (LLMs) {background-color="#40666e"}

## What are large language models (LLMs)
### Examples of large language models

![](Untitled design.png)

- Google Bert, 2017
- OpenAI's ChatGPT, 2022
- Meta's Llama-2, 2023

::: {.notes}
- ChatGPT first language model that became consumer technology
- Open source ones
:::

## What are large language models (LLMs)

![](Pasted image 20230825094408.png)

### Definitions
 
- LLMs model text and can be used to generate it
- LLMs receive large-scale **pretraining**
- LLMs make inferences based on **transfer learning**

::: {style="font-size: 50%;"}
Source: Luccioni, Alexandra Sasha, and Anna Rogers. “Mind Your Language (Model): Fact-Checking LLMs and Their Role in NLP Research and Practice.”
:::

::: {.notes}
- Pre-training: Large organisations
- They are good on a wide variety of tasks, categorisation, extracting entities
:::


## What are large language models (LLMs)
### A change in researcher workflow

|                       | Training your own model                                               | Text analysis with LLMs |
|-----------------------|-----------------------------------------------------------------------|------------------------------------------|
| **Models used**       | Researcher experiments with a range of model types specific to the task | Choosing a pre-trained LLM which may work on several tasks 
| **Preparing model**   | Training model exclusively on your data | Model has been pre-trained on large corpus, finetune on your own data                  |
: {tbl-colwidths="[20,40,40]"}

- LLMs are like *swiss army knives*, a decent solution for many tasks
- A potential simplification in expertise required for modeling - but at a cost!

::: {.notes}
- Typically, when working with large datasets, a researcher would have to find a specific model
- Language models are decent at many things - standardisation in the modelling phase of research
- Creates other challenges - dependency on companies that pretrain models, ethical dilemmas
:::


# Challenges in hate speech research {background-color="#40666e"}

## Challenges in hate speech detection
### Creating your dataset

- "Hate speech" definitions vary, and research often adopts context-specific notions (eg. extreme speech, harmful speech, othering speech, nethate)
- Bias in training data, for instance minority slang more often labelled as hate speech
- Low inter-coder reliability in coding

::: {.notes}
- Common pattern: 
- 
- Language models are decent at many things - standardisation in the modelling phase of research
- Creates other challenges - dependency on companies that pretrain models, ethical dilemmas
:::


## Challenges in hate speech detection
### Models won't replace human readers

- Core challenge for analysis is finding analytical concepts that
  1. Provide analytical value, i.e. reveal something insightful about data
  2. Are clear enough so that multiple people will agree on them
  3. Can be operationalised so that models such as LLMs can apply them

- Agreement between people is proxy for validity of concepts
- Finding right concepts may require many attempts, and LLMs may make iterative process easier

# Two concrete examples  {background-color="#40666e"}

##  Research of my own

![](Pasted image 20230830123704.png)

---

## Definition of "hateful speech"

![](Pasted image 20230823154547.png)

- Definition:
  1. Contains slur from pre-defined list
  2. Slur used in abusive or threatening way

<!--
Problematic, but quite transparent
-->

---

## Finetuning a model in Python

```
model = AutoModelForSequenceClassification.from_pretrained(
    "TurkuNLP/bert-base-finnish-uncased-v1"
)

training_args = TrainingArguments(
    "nethate", evaluation_strategy="epoch", logging_steps=30
)
metric = load_metric("accuracy")

trainer = Trainer(
    args=training_args,
    model=model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)
trainer.train()

finetuned_finbert = pipeline(
    model=model, tokenizer=tokenizer, task="sentiment-analysis", return_all_scores=True
)
```

Goal: A "boring", boilerplate model

::: {style="font-size: 50%;"}
Source: https://github.com/AleksiKnuutila/nethate_classifier/
::: 

<!--
Very simple and stupid, obvious, reproducible
Boilerplate
The interesting stuff isn't happening here
-->

## An alternative workflow: zero-shot annotation

- Can be done with LLMs that are trained to be directed with prompts
- Zero-shot: When the model is asked to perform a task that it hasn't specifically been trained on, i.e. to apply new categories that are described to it
- Results tend to be worse, but perhaps good enough for experimenting with concepts?

::: {style="font-size: 50%;"}
Source: Pangakis et al. "Automated Annotation with Generative AI Requires Validation"
::: 

---

## Example of zero-shot annotation

![](Pasted image 20230824094514.png)

An example of annotating text: Assessing the amount of time passing in literary passages.

::: {style="font-size: 50%;"}
Source: Underwood, "Why Literary Time is Measured in Minutes"
::: 

<!--
Literary-critical tradition suggested that there had been a pretty stable balance between “scene” (minute-by-minute description) and “summary” (which may cover weeks or years) until modernists started to leave out the summary and make every page breathlessly immediate
-->

---

## Prompting GPT-4 to read fiction
### Instructions in natural language

```
Read the following passage of fiction. Then do five things.
 
1. Reason step by step to decide how much time is described in the passage. If the
passage doesn't include any explicit reference to time, you can guess how much 
time the events described would have taken.

2. ...

The passage follows: ...
```


::: {style="font-size: 50%;"}
Source: https://tedunderwood.com/2023/03/19/using-gpt-4-to-measure-the-passage-of-time-in-fiction/
::: 

---

## Prompting GPT-4 to read fiction
### Results

```
Making coffee, showering, and getting dressed take at least an hour. There's 
some ambiguity about whether to count the implicit reference to yesterday 
(since this is 'the next morning') as time elapsed in the passage, but let's 
say no, since yesterday is not actually described. So, an hour to 90 minutes.
```

In the future, a way to pilot analysis with natural language prompts?

# Summary and conclusions {background-color="#40666e"}

---
## Conclusions


:::: {.columns}

::: {.column width="50%"}
- There is some hope that LLMs will reduce technical expertise required to work with large datasets
- This will come at cost of new dependencies, ethical issues, changing workflows
- The challenge of finding the right analytical concepts will remain, particularly when working on topics like hate speech
:::

::: {.column width="50%" style="font-size: 50%;"}
![](Pasted image 20230824121403.png)

Source: Tracy, "A Phronetic Iterative Approach to Data Analysis in Qualitative Research"
:::

::::


## Thanks! Questions and feedback appreciated {background-color="#40666e"}

aleksi.knuutila@helsinki.fi

<http://knuutila.net/methodfest2023>