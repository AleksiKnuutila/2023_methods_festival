---
title: "Language models as research tools: Potentials and pitfalls when studying hate speech"
subtitle: Methods festival 2023
format:
  clean-revealjs:
    self-contained: true
    template-partials:
      - title-slide.html
date: last-modified
bibliography: refs.bib
slide_link: http://knuutila.net/methodsfest2023
author:
  - name: Aleksi Knuutila
    orcid: 0000-0002-9874-0079
    email: aleksi.knuutila@helsinki.fi
    affiliations: JYU, HY
---

## A bit about me

- University researcher in Helsinki (Sociology) and Jyväskylä (Culture studies)
- Working on information environment in Ukraine, hate speech in Finland
- Interested in critical and creative use of machine learning in social sciences

## Structure of presentation

1. What are large language models ("LLMs")
2. Challenges in hate speech research
3. Examples of my own research and novel uses of LLMs
4. Concluding summary

# What are large language models (LLMs) {background-color="#40666e"}

## What are large language models (LLMs)
### Examples of large language models

![](Untitled design.png)

- Google Bert, 2017
- ChatGPT, 2022
- Llama-2, 2023

## What are large language models (LLMs)
### Definitions

![](Pasted image 20230825094408.png)
 
- LLMs model text and can be used to generate it
- LLMs receive large-scale **pretraining**
- LLMs make inferences based on **transfer learning**

Source: Luccioni, Alexandra Sasha, and Anna Rogers. “Mind Your Language (Model): Fact-Checking LLMs and Their Role in NLP Research and Practice.”

## What are large language models (LLMs)
### A change in researcher workflow

|                       | Training your own model                                               | Text analysis with LLMs |
|-----------------------|-----------------------------------------------------------------------|------------------------------------------|
| **Models used**       | Researcher experiments with a range of model types specific to the task | Choosing a pre-trained LLM which may work on several tasks 
| **Preparing model**   | Training model exclusively on your data | Model has been pre-trained on large corpus, finetune on your own data                  |

- Pre-training reduces data requirements
- LLMs are like *swiss army knives* - a potential simplification in required expertise

<!--
| **Data requirements** | Training and validation dataset (potentially large dataset necessary) | Training and validation dataset          |
-->

# Challenges in hate speech research {background-color="#40666e"}

### Context: Hate speech detection industry

- Google, Meta, Microsoft: Large tech companies leaders in development of new LLMs
- "Safety Tech" also a nascent commercial industry
- Hate speech detection is evaluated in terms of accuracy, and definition of hate speech taken as a given

<!--
Could talk about Gillespie here and how hate speech detection is an existential question for platforms
-->
## Challenges in hate speech detection
### 

- "Hate speech" definitions vary, and research often adopts context-specific notions (eg. extreme speech, harmful speech, othering speech)
- Bias in training data, for instance minority slang more often labelled as hate speech
- Low inter-coder reliability in coding

## Challenges in hate speech detection
### 

- Core challenge for analysis is finding analytical concepts that
1. Provide analytical value, i.e. reveal something insightful about data
2. Are clear enough so that multiple people will agree on them
3. Can be operationalised so that models such as LLMs can apply them

- Agreement between people is proxy for validity of concepts - for some research, codebook verified before modelling
- Finding right concepts may require many attempts, and LLMs may make iterative process easier

# Examples from existing research

## Some of my own research

![[Pasted image 20230830123704.png]]

---

## Example of supervised text categorisation

![[Pasted image 20230823154547.png]]

- Definition:
  1. Contains slur from pre-defined list
  2. Slur used in abusive or threatening way

<!--
Problematic, but quite transparent
-->

---

## Finetuning a model in Python

```
model = AutoModelForSequenceClassification.from_pretrained(
    "TurkuNLP/bert-base-finnish-uncased-v1"
)

training_args = TrainingArguments(
    "nethate", evaluation_strategy="epoch", logging_steps=30
)
metric = load_metric("accuracy")

trainer = Trainer(
    args=training_args,
    model=model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)
trainer.train()

finetuned_finbert = pipeline(
    model=model, tokenizer=tokenizer, task="sentiment-analysis", return_all_scores=True
)
```

Source: https://github.com/AleksiKnuutila/nethate_classifier/blob/master/train.py

<!--
Very simple and stupid, obvious, reproducible
Boilerplate
The interesting stuff isn't happening here
-->

---

## Evaluating the performance of the classification

![[Pasted image 20230824091245.png]]

---
## An alternative workflow: zero-shot annotation

- Can be done with LLMs that are trained to be directed with prompts
- Zero-shot: When the model is asked to perform a task that it hasn't specifically been trained on, i.e. to apply new categories that are described to it
- Few-shot: When the model is provided with a limited number of examples for its task
- Results tend to be worse than with finetuned models

Source: Pangakis et al. "Automated Annotation with Generative AI Requires Validation"

---

## Example of zero-shot annotation

![[Pasted image 20230824094514.png]]

An example of annotating text: Assessing the amount of time passing in literary passages.

Source: Underwood, "Why Literary Time is Measured in Minutes"

<!--
Literary-critical tradition suggested that there had been a pretty stable balance between “scene” (minute-by-minute description) and “summary” (which may cover weeks or years) until modernists started to leave out the summary and make every page breathlessly immediate
-->

---

## Prompting GPT-4 to read fiction

```
Read the following passage of fiction. Then do five things.
 
Reason step by step to decide how much time is described in the passage. If the
passage doesn't include any explicit reference to time, you can guess how much 
time the events described would have taken.

The passage follows: ...
```

Source: https://tedunderwood.com/2023/03/19/using-gpt-4-to-measure-the-passage-of-time-in-fiction/

---

## Results

```
Making coffee, showering, and getting dressed take at least an hour. There's 
some ambiguity about whether to count the implicit reference to yesterday 
(since this is 'the next morning') as time elapsed in the passage, but let's 
say no, since yesterday is not actually described. So, an hour to 90 minutes.
```

---

## Findings

![[Pasted image 20230825100706.png]]
"Watching the model think out loud about each passage actually gave me a better sense of how results were produced than I could extract from my own records of human research in 2017."

<!--

## Thematic analysis with LLMs?

- XXX
- Possible source: https://arxiv.org/pdf/2305.13014.pdf
- Possible source: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4487768

-->

<!--
## Simple guidelines for "prompt engineering"

- Giving examples
- Providing persona
- Trying to get machine-readable output can be a challenge
-->

# Conclusions (LLMs) {background-color="#40666e"}

---
## Conclusions

:::: {.columns}

::: {.column width="50%"}
- There is some hope that LLMs will reduce technical expertise required to work with large datasets
- This will come at cost of new dependencies, ethical issues, changing workflows
- The challenge of finding the right analytical concepts will remain, particularly when working on topics like hate speech
:::

::: {.column width="50%"}
![](Pasted image 20230824121403.png)

Source: Tracy, "A Phronetic Iterative Approach to Data Analysis in Qualitative Research"
:::

::::


## Thanks! Questions and feedback appreciated

aleksi.knuutila@helsinki.fi

http://knuutila.net/methodfest2023