---
title: "Language models as research tools: Potentials and pitfalls when studying hate speech"
subtitle: Methods festival 2023
format:
  clean-revealjs:
    self-contained: true
    template-partials:
      - title-slide.html
author:
  - name: Aleksi Knuutila
    orcid: 0000-0002-9874-0079
    email: aleksi.knuutila@helsinki.fi
    affiliations: JYU, HY
date: last-modified
bibliography: refs.bib
slide_link: http://knuutila.net/mf2023
---
## A bit about me

- University researcher in Helsinki (Sociology) and Jyväskylä (Culture studies)
- Working on information environment in Ukraine, hate speech in Finland
- Interested in critical and creative use of machine learning in social sciences

::: {.notes}
- I've worked on hate speech for a few years now, collaborating with many people in this panel also
:::
## Structure of presentation

1. What are large language models ("LLMs")
2. Challenges in hate speech research
3. Examples of my own research and novel uses of LLMs
4. Concluding summary

::: {.notes}
- Teaching on LLMs, tried to connect it hate speech
- Research approach, based on categorisation of large datasets
:::

# What are large language models (LLMs) {background-color="#40666e"}

## What are large language models (LLMs)
### Examples of large language models

![](Untitled design.png)

- Google Bert, 2017
- OpenAI's ChatGPT, 2022
- Meta's Llama-2, 2023

::: {.notes}
- ChatGPT first language model that became consumer technology
- Open source ones
:::

## What are large language models (LLMs)

![](Pasted image 20230825094408.png)

### Definitions
 
- LLMs model text and can be used to generate it
- LLMs receive large-scale **pretraining**
- LLMs make inferences based on **transfer learning**

::: {style="font-size: 50%;"}
Source: Luccioni, Alexandra Sasha, and Anna Rogers. “Mind Your Language (Model): Fact-Checking LLMs and Their Role in NLP Research and Practice.”
:::

::: {.notes}
- Pre-training: Large organisations
- They are good on a wide variety of tasks, categorisation, extracting entities
:::


## What are large language models (LLMs)
### A change in researcher workflow

|                       | Training your own model                                               | Text analysis with LLMs |
|-----------------------|-----------------------------------------------------------------------|------------------------------------------|
| **Models used**       | Researcher experiments with a range of model types specific to the task | Choosing a pre-trained LLM which may work on several tasks 
| **Preparing model**   | Training model exclusively on your data | Model has been pre-trained on large corpus, finetune on your own data                  |
: {tbl-colwidths="[20,40,40]"}

- LLMs are like *swiss army knives*, a decent solution for many tasks
- A potential simplification in expertise required for modeling - but at a cost!

::: {.notes}
- Typically, when working with large datasets, a researcher would have to find a specific model
- Language models are decent at many things - standardisation in the modelling phase of research
- Creates other challenges - dependency on companies that pretrain models, ethical dilemmas
:::

# Challenges in hate speech research {background-color="#40666e"}

## Challenges in hate speech detection
### Creating your dataset

- "Hate speech" definitions vary, and researchers often adopt context-specific concepts (eg. extreme speech, harmful speech, othering speech, nethate)
- Low inter-coder reliability in coding (eg. about 70% on HASOC)
- Bias in training data, for instance minority slang more often labelled as hate speech

::: {.notes}
- Common pattern: research expresses an interest in hate speech, and then posits analytically tractable concepts
- This is probably true for many phenomena, but no wide consensus on analytical concepts in field of hate speech
- Even good analytical concepts might not mean that people apply them in the same way - for instance in labelling tasks
:::

## Challenges in hate speech detection
### Models won't replace human readers

- Core challenge is finding analytical concepts that
  1. Provide analytical value, i.e. reveal something insightful about data
  2. Are clear enough so that several readers agree on how to apply them
  3. Can be operationalised with models such as LLMs

- Agreement between people is proxy for validity of concepts
- Finding right concepts may require many attempts 
    - LLMs may make iterative process easier

::: {.notes}
- So in this type of research, where you apply categories on a large dataset, these are generally the 3 requirements you have for analytical concepts
- Tell you something interesting, people will agree on them, and then models can actually replicate the taxonomy
- No matter how technically advanced your model is, you still need the human readers
- Not everyone agrees, but its quite common to see this
- And anyone who has measured inter-coder reliability knows its not easy
- May take several tries
- And one potential of LLMs is that the cost of modelling will decrease, and so iterative experimentation of does this concept work with the model? what do the results look like with this concept? that becomes easier
:::

# Two concrete examples  {background-color="#40666e"}

## Research with Cabinet office

![](Pasted image 20230830123704.png)

::: {.notes}
- Some collaborations with people in the panel
- Two projects funded by Cabinet office
- Research questions such as: which politicians receive abuse on social media, that require this kind of 
:::

---

## Definition of "hateful speech"

![](Pasted image 20230823154547.png)

**hateful speech**: Messages that 

  1. Contains slur from pre-defined list
  2. Use slurs used in abusive or threatening way
  
::: {.notes}
- Problematic, rather static using a predefined word list
- For public-facing project, there's value having rather clear definition, and focusing on messages that are quite unambiguously wrong
:::

---

## Finetuning a model in Python

```
model = AutoModelForSequenceClassification.from_pretrained(
    "TurkuNLP/bert-base-finnish-uncased-v1"
)

training_args = TrainingArguments(
    "nethate", evaluation_strategy="epoch", logging_steps=30
)
metric = load_metric("accuracy")

trainer = Trainer(
    args=training_args,
    model=model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)
trainer.train()

finetuned_finbert = pipeline(
    model=model, tokenizer=tokenizer, task="sentiment-analysis", return_all_scores=True
)
```

Goal: A "boring", boilerplate model

::: {.notes}
- So this is Python code for finetuning a language model
- This might seem complex, but default, few parameters changed
- I thought for this work, it's appropriate to have an almost "boring" model
- Embodies the idea that the value is created through labour on the dataset and finding the right analytical angles, not through some technical aspects of the AI
:::

::: {style="font-size: 50%;"}
Source: https://github.com/AleksiKnuutila/nethate_classifier/
::: 

<!--
Very simple and stupid, obvious, reproducible
Boilerplate
The interesting stuff isn't happening here
-->

## An alternative workflow: zero-shot annotation

- Can be done with LLMs that are trained to be directed with prompts
- Zero-shot: When the model is asked to perform a task that it hasn't specifically been trained on, i.e. to apply new categories that are described to it
- Results tend to be worse, but perhaps good enough for experimenting with concepts?

::: {style="font-size: 50%;"}
Source: Pangakis et al. "Automated Annotation with Generative AI Requires Validation"
::: 

::: {.notes}
- And very briefly, there's a way to use language models that is kind of nascent, some scientists have used it, but not clear what the limits are
- Zero-shot: You don't need a training set to direct your model, but instead you can instruct it how to do categorisation, potentially with examples
- Potential for kind of quick experiments
:::

---

## Example of zero-shot annotation

![](Pasted image 20230824094514.png)

An example of annotating text: Assessing the amount of time passing in literary passages.

::: {style="font-size: 50%;"}
Source: Underwood, "Why Literary Time is Measured in Minutes"
::: 

<!--
Literary-critical tradition suggested that there had been a pretty stable balance between “scene” (minute-by-minute description) and “summary” (which may cover weeks or years) until modernists started to leave out the summary and make every page breathlessly immediate
-->

---

## Prompting GPT-4 to read fiction
### Instructions in natural language

```
Read the following passage of fiction. Then do five things.
 
1. Reason step by step to decide how much time is described in the passage. If the
passage doesn't include any explicit reference to time, you can guess how much 
time the events described would have taken.

2. ...

The passage follows: ...
```


::: {style="font-size: 50%;"}
Source: https://tedunderwood.com/2023/03/19/using-gpt-4-to-measure-the-passage-of-time-in-fiction/
::: 

---

## Prompting GPT-4 to read fiction
### Results

```
Making coffee, showering, and getting dressed take at least an hour. There's 
some ambiguity about whether to count the implicit reference to yesterday 
(since this is 'the next morning') as time elapsed in the passage, but let's 
say no, since yesterday is not actually described. So, an hour to 90 minutes.
```

In the future, a way to pilot analysis with natural language prompts?

# Summary and conclusions {background-color="#40666e"}

---
## Conclusions


:::: {.columns}

::: {.column width="50%"}
- Challenge of finding right analytical concepts particularly prevalent in hate speech research
- Previously the high cost of modelling meant concept development done on subset of data, before modelling
- LLMs may reduce cost of modelling and make it easier to iteratively read large datasets with novel concepts
- LLMs introduce their own ethical issues and dependencies
:::

::: {.column width="50%" style="font-size: 50%;"}
![](Pasted image 20230824121403.png)

Source: Tracy, "A Phronetic Iterative Approach to Data Analysis in Qualitative Research"
:::

::::


::: {.notes}
Bringing it together
Finding the right concepts is hard
Language models don't do this work for you, but they might provide new workflows for large datasets
Previously research often in consecutive phases, first you agree on concepts, then you create the model
The modelling is so costly that you can't repeat it
Ideally you might try out whether the model can repeat your taxonomy, and see what kind of insights are gained
And repeat this process
Visualised, Tracy, phronetic approach, hermeneutic circle, defined in contrast with grounded theory..
:::

## Thanks! Questions and feedback appreciated {background-color="#40666e"}

aleksi.knuutila@helsinki.fi

<http://knuutila.net/mf2023>